{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22700f2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you haven't already, install the toolkit and dependencies using the [Setup](./00-Setup.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa5a0dd",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The [Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) is an open protocol that standardizes how applications provide context to LLMs.\n",
    "\n",
    "In this example, we're going to create a 'catalog' of tools, one per tenant in a [multi-tenant](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/multi-tenancy.md) graph. Each tool is capable of answering domain-specific questions based on the data in its tenant graph. This catalog will be advertised to clients via an MCP server. Clients (typically agents and LLMs) can then browse the catalog and choose appropriate tools for addressing their information goals.\n",
    "\n",
    "Each tool in the catalog is accompanied by an auto-generated description that helps a client understand the domain, scope, potential uses and kinds of questions covered by the tool. The catalog also includes a 'search' tool, which, given the name of an entity or concept, recommends one or more domain tools with knowledge of the search term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8788590",
   "metadata": {},
   "source": [
    "### Additional setup\n",
    "\n",
    "#### Create tenant-specific lexical graphs\n",
    "\n",
    "Besides the initial setup described above, this example requires you to have created two tenant-specific lexical graphs: a Neptune documentation lexical graph (installed in the _default_ tenant graph), and an Amazon OpenSearch documentation lexical graph (installed in the `aoss` tenant graph):\n",
    "\n",
    "  - To create the Neptune documentation lexical graph, run either notebook [01-Combined-Extract-and-Build](./01-Combined-Extract-and-Build.ipynb) or notebook [02-Separate-Extract-and-Build](./02-Separate-Extract-and-Build.ipynb).\n",
    "  - To create the Amazon OpenSearch documentation lexical graph, run the create cell in notebook [05-Multi-Tenancy](./05-Multi-Tenancy.ipynb)\n",
    "\n",
    "#### Provide Bedrock model access for Claude 3.7 Sonnet\n",
    "\n",
    "Ensure you have [requested access](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html) in Amazon Bedrock to the `anthropic.claude-3-7-sonnet-20250219-v1:0` foundation model.\n",
    "\n",
    "#### Install additional dependencies\n",
    "\n",
    "The last thing you need to do is install these additional dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2701e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastmcp strands-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf844b9",
   "metadata": {},
   "source": [
    "## Create an MCP server\n",
    "\n",
    "The following cell creates an MCP server that hosts a catalog of tools – one per tenant graph. The cell takes a few seconds to run while the tool descriptions are auto-generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6762d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory, VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.protocols import create_mcp_server\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "mcp_server = create_mcp_server(graph_store, vector_store)\n",
    "\n",
    "print('Server initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29f4ee",
   "metadata": {},
   "source": [
    "### Start the server\n",
    "\n",
    "The cell below starts the MCP server using the Streamable HTTP transport on a background thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99abadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def run_server():\n",
    "    mcp_server.run(transport='streamable-http', log_level='warning')\n",
    "    \n",
    "thread = threading.Thread(target=run_server)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad5e09",
   "metadata": {},
   "source": [
    "## Create an MCP client and AI agent\n",
    "\n",
    "[Strands Agents](https://strandsagents.com/latest/) is an open source SDK that takes a model-driven approach to building and running AI agents in just a few lines of code.\n",
    "\n",
    "In the cell below we create an MCP client that we can then use in a Strands Agent for answering cross-domain questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from strands.tools.mcp.mcp_client import MCPClient\n",
    "\n",
    "def create_streamable_http_transport():\n",
    "    return streamablehttp_client('http://localhost:8000/mcp/')\n",
    "\n",
    "mcp_client = MCPClient(create_streamable_http_transport)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f6bc50",
   "metadata": {},
   "source": [
    "### Inspect the tool descriptions\n",
    "\n",
    "The code below prints out the tool descriptions that have been auto-generated from the tenant graphs. Each tool is named after its tenant. The tool for the default tenant graph is named `tenant_`. Note that besides the tenant-specific tools, there is also a `search_` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f962745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "\n",
    "with mcp_client:\n",
    "    \n",
    "    tools = mcp_client.list_tools_sync()\n",
    "    \n",
    "    for tool in tools:\n",
    "        print(f\"{tool.tool_spec['name']}: {tool.tool_spec['description']}\")\n",
    "        print('\\n-------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911409f5",
   "metadata": {},
   "source": [
    "### Create an agent and ask a question\n",
    "\n",
    "We can now create a Strands AI Agent, and ask a question. The agent will choose the most appropriate tools for answering the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a023070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mcp_client:\n",
    "\n",
    "    tools = mcp_client.list_tools_sync()\n",
    "    agent = Agent(tools=tools)\n",
    "    \n",
    "    response = agent(\"What are the differences between Amazon Neptune and Amazon OpenSearch Serverless?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
