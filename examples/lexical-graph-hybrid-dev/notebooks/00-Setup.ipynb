{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65a1effc",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd436a",
   "metadata": {},
   "source": [
    "Install the graphrag-toolkit-lexical-graph:"
   ]
  },
  {
   "cell_type": "code",
   "id": "a3947399",
   "metadata": {},
   "source": "!pip install https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.6.1.zip#subdirectory=lexical-graph",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.6.0.zip#subdirectory=lexical-graph-contrib/falkordb",
   "id": "ea6b28938892cfeb"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql1"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%%sql\n",
   "id": "677a1e2e2d427131"
  },
  {
   "cell_type": "markdown",
   "id": "d0c4cfb7",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82508322",
   "metadata": {},
   "source": [
    "### Amazon OpenSearch Serverless\n",
    "\n",
    "If you are using Amazon OpenSearch Serverless as a vector store, install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "id": "48697a86",
   "metadata": {},
   "source": [
    "!pip install opensearch-py llama-index-vector-stores-opensearch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d043070",
   "metadata": {},
   "source": [
    "### Postgres with pgvector\n",
    "\n",
    "If you are using a Postgres database with the pgvector extension as a vector store, install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "id": "cb0cd0a5",
   "metadata": {},
   "source": [
    "!pip install psycopg2-binary pgvector"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d2a8c967",
   "metadata": {},
   "source": [
    "## Additional dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49587ec",
   "metadata": {},
   "source": [
    "### LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f329a",
   "metadata": {},
   "source": [
    "Install any necessary LlamaIndex reader depedencies. For the examples in these notebooks, you'll need the Web readers:"
   ]
  },
  {
   "cell_type": "code",
   "id": "f439752b",
   "metadata": {},
   "source": [
    "!pip install llama-index-readers-web"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5a46d96e",
   "metadata": {},
   "source": [
    "### Rerankers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb92c81",
   "metadata": {},
   "source": [
    "If you intend to use the `SentenceReranker` with the `TraversalBasedRetriever` (see the details [here](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/querying.md#statement-reranking)), install the following dependencies: "
   ]
  },
  {
   "cell_type": "code",
   "id": "a704c21d",
   "metadata": {},
   "source": [
    "!pip install torch sentence_transformers"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e89630b",
   "metadata": {},
   "source": [
    "If you intend to use the `BGEReranker` with a GPU machine (see the details [here](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/querying.md#semanticguidedretriever-with-a-reranking-beam-search)), install the following dependencies: "
   ]
  },
  {
   "cell_type": "code",
   "id": "67f7d824",
   "metadata": {},
   "source": [
    "!pip install torch FlagEmbedding"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "PyMuPDF: This is a Python binding for MuPDF, a lightweight PDF, XPS, and E-book viewer, renderer, and toolkit.\n",
    "- Read and modify PDF files\n",
    "- Extract text and images from PDFs\n",
    "- Manipulate PDF documents programmatically\n",
    "- Compress PDF files\n",
    "- Handle other document formats like XPS, EPUB, etc.\n",
    "\n",
    "llama-index[pdf]: This is the PDF extension of LlamaIndex (formerly GPT Index), which is a data framework for LLM (Large Language Model) applications. The [pdf] suffix indicates that you're installing the PDF-specific dependencies.\n",
    "- Create structured data from PDF documents\n",
    "- Build indexes from PDF content\n",
    "- Query and retrieve information from PDF documents\n",
    "- Integrate PDF content with LLM applications"
   ],
   "id": "4a9bec2605c060b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install PyMuPDF llama-index[pdf]",
   "id": "1d1c2650a1aee820"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
