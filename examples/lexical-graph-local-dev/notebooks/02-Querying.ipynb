{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Querying",
   "id": "3bfeb79c9431a3c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T18:30:19.929672Z",
     "start_time": "2025-05-29T18:30:17.831987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.falkordb import FalkorDBGraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph import set_logging_config\n",
    "\n",
    "set_logging_config('INFO')\n",
    "\n",
    "# Register the FalkorDB backend with the factory\n",
    "GraphStoreFactory.register(FalkorDBGraphStoreFactory)\n",
    "\n",
    "# Create graph and vector stores\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])"
   ],
   "id": "6edfdf2594abab7c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evanerwee/Prod/graphrag-toolkit/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### SemanticGuidedRetriever\n",
    "\n",
    "See [SemanticGuidedRetriever](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/querying.md#semanticguidedretriever)."
   ],
   "id": "310c60207b80a332"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.retrieval.retrievers import RerankingBeamGraphSearch, StatementCosineSimilaritySearch, KeywordRankingSearch\n",
    "from graphrag_toolkit.lexical_graph.retrieval.post_processors import SentenceReranker\n",
    "\n",
    "cosine_retriever = StatementCosineSimilaritySearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "keyword_retriever = KeywordRankingSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    max_keywords=10\n",
    ")\n",
    "\n",
    "reranker = SentenceReranker(\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "beam_retriever = RerankingBeamGraphSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    reranker=reranker,\n",
    "    initial_retrievers=[cosine_retriever, keyword_retriever],\n",
    "    max_depth=8,\n",
    "    beam_width=100\n",
    ")\n",
    "\n",
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(\n",
    "    graph_store,\n",
    "    vector_store,\n",
    "    retrievers=[\n",
    "        cosine_retriever,\n",
    "        keyword_retriever,\n",
    "        beam_retriever\n",
    "    ],\n",
    "\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the similarities and differences between Neptune Database and Neptune Analytics?\")\n",
    "\n",
    "print(response.response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set prompt from disk",
   "id": "8c9a1e5f27a4df28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.prompts.file_prompt_provider import FilePromptProvider\n",
    "from graphrag_toolkit.lexical_graph.prompts.prompt_provider_config import FilePromptProviderConfig\n",
    "from graphrag_toolkit.lexical_graph.retrieval.retrievers import RerankingBeamGraphSearch, StatementCosineSimilaritySearch, KeywordRankingSearch\n",
    "from graphrag_toolkit.lexical_graph.retrieval.post_processors import SentenceReranker\n",
    "\n",
    "# Step 1: Setup your prompt provider config (pointing to your prompt files)\n",
    "prompt_provider = FilePromptProvider(\n",
    "    FilePromptProviderConfig(\n",
    "        system_prompt_file=\"prompts/system.txt\",\n",
    "        user_prompt_file=\"prompts/user.txt\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 2: Setup your retrievers\n",
    "cosine_retriever = StatementCosineSimilaritySearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "keyword_retriever = KeywordRankingSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    max_keywords=10\n",
    ")\n",
    "\n",
    "reranker = SentenceReranker(batch_size=128)\n",
    "\n",
    "beam_retriever = RerankingBeamGraphSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    reranker=reranker,\n",
    "    initial_retrievers=[cosine_retriever, keyword_retriever],\n",
    "    max_depth=8,\n",
    "    beam_width=100\n",
    ")\n",
    "\n",
    "# Step 3: Instantiate the query engine with prompt_provider\n",
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(\n",
    "    graph_store,\n",
    "    vector_store,\n",
    "    retrievers=[cosine_retriever, keyword_retriever, beam_retriever],\n",
    "    prompt_provider=prompt_provider\n",
    ")\n",
    "\n",
    "# Step 4: Run your query\n",
    "response = query_engine.query(\"What are the similarities and differences between Neptune Database and Neptune Analytics?\")\n",
    "print(response.response)\n"
   ],
   "id": "d74997bf365edc3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set prompt from S3",
   "id": "fbea109ac64c970a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T19:30:52.220979Z",
     "start_time": "2025-05-29T19:30:31.793977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.prompts.s3_prompt_provider import S3PromptProvider\n",
    "from graphrag_toolkit.lexical_graph.prompts.prompt_provider_config import S3PromptProviderConfig\n",
    "from graphrag_toolkit.lexical_graph.retrieval.retrievers import (\n",
    "    RerankingBeamGraphSearch,\n",
    "    StatementCosineSimilaritySearch,\n",
    "    KeywordRankingSearch\n",
    ")\n",
    "from graphrag_toolkit.lexical_graph.retrieval.post_processors import SentenceReranker\n",
    "\n",
    "# Step 1: Setup your S3 prompt provider\n",
    "prompt_provider = S3PromptProvider(\n",
    "    S3PromptProviderConfig(\n",
    "        bucket=\"ccms-prompts\",\n",
    "        prefix=\"prompts\",\n",
    "        aws_region=\"ap-south-1\",  # optional if not using env\n",
    "        aws_profile=\"padmin\",\n",
    "        system_prompt_file=\"system_prompt.txt\",\n",
    "        user_prompt_file=\"user_prompt.txt\",# optional if not using default\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 2: Setup your retrievers\n",
    "cosine_retriever = StatementCosineSimilaritySearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "keyword_retriever = KeywordRankingSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    max_keywords=10\n",
    ")\n",
    "\n",
    "reranker = SentenceReranker(batch_size=128)\n",
    "\n",
    "beam_retriever = RerankingBeamGraphSearch(\n",
    "    vector_store=vector_store,\n",
    "    graph_store=graph_store,\n",
    "    reranker=reranker,\n",
    "    initial_retrievers=[cosine_retriever, keyword_retriever],\n",
    "    max_depth=8,\n",
    "    beam_width=100\n",
    ")\n",
    "\n",
    "# Step 3: Instantiate the query engine with the S3-based prompt provider\n",
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(\n",
    "    graph_store,\n",
    "    vector_store,\n",
    "    retrievers=[cosine_retriever, keyword_retriever, beam_retriever],\n",
    "    prompt_provider=prompt_provider\n",
    ")\n",
    "\n",
    "# Step 4: Run your query\n",
    "response = query_engine.query(\"What are the similarities and differences between Neptune Database and Neptune Analytics?\")\n",
    "print(response.response)\n"
   ],
   "id": "33b70a11bca9580d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-29 15:30:32:INFO:g.l.p.s3_prompt_provider:[Prompt Debug] Loading prompt from S3: s3://ccms-prompts/prompts/system_prompt.txt\n",
      "2025-05-29 15:30:34:INFO:g.l.p.s3_prompt_provider:[Prompt Debug] Loading prompt from S3: s3://ccms-prompts/prompts/user_prompt.txt\n",
      "2025-05-29 15:30:39:INFO:g.l.r.r.rerank_beam_search:Retrieved 21 new nodes through beam search.\n",
      "{\n",
      "  \"answer\": \"Neptune Database and Neptune Analytics are both part of Amazon Neptune but serve different purposes. Neptune Database is a serverless graph database designed for operational workloads, while Neptune Analytics is an analytics engine for processing and analyzing large graph datasets. They have similarities in dealing with graph data but differ in their primary functions, performance characteristics, and use cases.\",\n",
      "  \"supporting_facts\": [\n",
      "    \"Neptune Database is a serverless graph database designed for optimal scalability and availability [source_1]\",\n",
      "    \"Neptune Analytics is an analytics database engine for analyzing graph databases and datasets [source_1]\",\n",
      "    \"Neptune Database can scale to 100,000 queries per second and is used for applications like fraud alerting and social networking [source_1]\",\n",
      "    \"Neptune Analytics can process thousands of analytic queries per second and is used for getting insights, finding trends, and analyzing large amounts of graph data quickly [source_1]\",\n",
      "    \"Neptune Analytics complements Neptune Database [source_2]\",\n",
      "    \"Neptune Analytics can load data from Neptune Database graphs and snapshots [source_2]\",\n",
      "    \"Neptune Analytics is memory-optimized and stores large graph datasets in memory for quick analysis [source_2]\",\n",
      "    \"Neptune Database provides Multi-AZ high availability and supports multi-Region deployments [source_1]\",\n",
      "    \"Neptune Analytics supports a library of optimized graph analytic algorithms and vector search capabilities [source_2]\",\n",
      "    \"Both are part of Amazon Neptune, which is a fully managed graph database service [source_3]\"\n",
      "  ],\n",
      "  \"confidence\": \"high\"\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Bedrock Manage Prompt",
   "id": "389cc603c4056841"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "845b89c30a9a71b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
